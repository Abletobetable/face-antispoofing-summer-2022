{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "79TWUE15JXzi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a19b89c69ef4883980b8af23507da4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80f957ed590c4e1f9b4996ab3b907714",
              "IPY_MODEL_32f2a17acbaf4fb7a46fb10e4593aa90"
            ],
            "layout": "IPY_MODEL_6c2452a01a3f42bab31213b28669077a"
          }
        },
        "80f957ed590c4e1f9b4996ab3b907714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79d7887e8bce4523ac683a42710463d4",
            "placeholder": "​",
            "style": "IPY_MODEL_aa08823834e34c8aa055d24133b63139",
            "value": "0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "32f2a17acbaf4fb7a46fb10e4593aa90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62332f37b65042a0bea8d1b4c93b8c6a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61a797b7434c4d5687be461ddaf6506b",
            "value": 1
          }
        },
        "6c2452a01a3f42bab31213b28669077a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79d7887e8bce4523ac683a42710463d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa08823834e34c8aa055d24133b63139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62332f37b65042a0bea8d1b4c93b8c6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a797b7434c4d5687be461ddaf6506b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports, prepocessing functions and login to wandb"
      ],
      "metadata": {
        "id": "RBGFmjF3Iwi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wi7BnuSWA5pS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import feature as skif\n",
        "import cv2\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from zipfile import ZipFile\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -Uq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiTPZyGtBGTx",
        "outputId": "d7138764-d55f-4415-88f4-e39b4d32e317"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 62.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 53.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 61.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 71.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 72.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 71.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 68.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 71.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 66.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MYaTwi_zBIIw",
        "outputId": "b7c8a28f-124c-48cf-cd6b-649d24ec8f8d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_partition_and_labels(data):\n",
        "    \"\"\"\n",
        "    creates 2 dictionary like follow:\n",
        "\n",
        "    >>> partition\n",
        "    {'train': ['path-1', 'path-2', 'path-3'], 'validation': ['path-4']}\n",
        "\n",
        "    >>> ID_labels\n",
        "    {'path-1': 0, 'path-2': 1, 'path-3': 2, 'path-4': 1}\n",
        "    \"\"\"\n",
        "    train = []\n",
        "    train_label = []\n",
        "    test = []\n",
        "    test_label = []\n",
        "\n",
        "    # test data\n",
        "    for folder in os.listdir(f'/content/{data}/test'):\n",
        "        if os.path.exists(f'/content/{data}/test/'+folder+'/live'):\n",
        "            for image in os.listdir(f'/content/{data}/test/'+folder+'/live'):\n",
        "                test.append(f'/content/{data}/test/'+folder+'/live/'+image)\n",
        "                test_label.append(0)\n",
        "        if os.path.exists(f'/content/{data}/test/'+folder+'/spoof'):\n",
        "            for image in os.listdir(f'/content/{data}/test/'+folder+'/spoof'):\n",
        "                test.append(f'/content/{data}/test/'+folder+'/spoof/'+image)\n",
        "                test_label.append(1)\n",
        "    \n",
        "    # train data\n",
        "    for folder in os.listdir(f'/content/{data}/train'):\n",
        "        if os.path.exists(f'/content/{data}/train/'+folder+'/live'):\n",
        "            for image in os.listdir(f'/content/{data}/train/'+folder+'/live'):\n",
        "                train.append(f'/content/{data}/train/'+folder+'/live/'+image)\n",
        "                train_label.append(0)\n",
        "        if os.path.exists(f'/content/{data}/train/'+folder+'/spoof'):\n",
        "            for image in os.listdir(f'/content/{data}/train/'+folder+'/spoof'):\n",
        "                train.append(f'/content/{data}/train/'+folder+'/spoof/'+image)\n",
        "                train_label.append(1)\n",
        "    \n",
        "    # train test valid split\n",
        "    train, valid, train_label, valid_label = train_test_split(train, train_label, \n",
        "                                                              train_size = 0.8, shuffle=True, random_state=44)\n",
        "    partition = {'test': test, 'train': train, 'valid': valid}\n",
        "\n",
        "    # path <-> image label\n",
        "    labels_list = test_label + train_label + valid_label\n",
        "    path_list = test + train + valid\n",
        "    path_labels = dict(zip(path_list, labels_list))\n",
        "\n",
        "    return partition, path_labels"
      ],
      "metadata": {
        "id": "2EOhxLQMGDOn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, path_list, id_labels):\n",
        "        self.path_list = path_list\n",
        "        self.id_labels = id_labels\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.path_list)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        def lbp_histogram(image,P=8,R=1,method = 'nri_uniform'):\n",
        "            '''\n",
        "            image: shape is N*M \n",
        "            '''\n",
        "            lbp = skif.local_binary_pattern(image, P, R, method)\n",
        "            max_bins = int(lbp.max() + 1)\n",
        "            hist,_= np.histogram(lbp, density=True, bins=max_bins, range=(0, max_bins))\n",
        "            return hist\n",
        "\n",
        "        def get_features(file_name):\n",
        "            image = cv2.imread(file_name)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
        "            y_h = lbp_histogram(image[:,:,0]) # y channel\n",
        "            cb_h = lbp_histogram(image[:,:,1]) # cb channel\n",
        "            cr_h = lbp_histogram(image[:,:,2]) # cr channel\n",
        "            feature = np.concatenate((y_h,cb_h,cr_h))\n",
        "            return feature\n",
        "        def padding(image_vector, to_size = 177):\n",
        "            if len(image_vector) < to_size:\n",
        "                image_vector = np.append(image_vector, np.zeros(to_size-len(image_vector)))\n",
        "            return image_vector\n",
        "\n",
        "        image_path = self.path_list[index]\n",
        "        image_vector = get_features(image_path)\n",
        "\n",
        "        X = padding(image_vector)\n",
        "        y = self.id_labels[image_path]\n",
        "\n",
        "        return torch.Tensor(X), torch.tensor(y, dtype=torch.long)"
      ],
      "metadata": {
        "id": "SNYnTCagGcli"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "rAEi9hdwI6gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')\n",
        "\n",
        "DATASET_NAME = 'live'\n",
        "with ZipFile(f'/content/gdrive/MyDrive/{DATASET_NAME}.zip', 'r') as dataset_zip:\n",
        "    dataset_zip.extractall('/content')\n",
        "\n",
        "DATASET_NAME_2 = 'replay_attack'\n",
        "with ZipFile(f'/content/gdrive/MyDrive/{DATASET_NAME_2}.zip', 'r') as dataset_zip:\n",
        "    dataset_zip.extractall('/content')\n",
        "\n",
        "DATASET_NAME_3 = 'print_attack'\n",
        "with ZipFile(f'/content/gdrive/MyDrive/{DATASET_NAME_3}.zip', 'r') as dataset_zip:\n",
        "    dataset_zip.extractall('/content')\n",
        "\n",
        "try: ### IF TO TRAIN ###\n",
        "\n",
        "    # get list with path to images and their labels\n",
        "    partition, path_labels = create_partition_and_labels(data = DATASET_NAME)\n",
        "\n",
        "    # to torch format\n",
        "    training_set = MyDataset(partition['train'], path_labels)\n",
        "    validation_set = MyDataset(partition['valid'], path_labels)\n",
        "    testing_set = MyDataset(partition['test'], path_labels)\n",
        "\n",
        "except: ### IF ONLY TO TEST ###\n",
        "\n",
        "    test_list = []\n",
        "\n",
        "    # live\n",
        "    for image in sorted(os.listdir(DATASET_NAME)):\n",
        "        test_list.append(f'{DATASET_NAME}/{image}')\n",
        "    len_live = len(test_list)\n",
        "\n",
        "    # replay\n",
        "    for image in sorted(os.listdir(DATASET_NAME_2)):\n",
        "        test_list.append(f'{DATASET_NAME_2}/{image}')\n",
        "\n",
        "    # print\n",
        "    for image in sorted(os.listdir(DATASET_NAME_3)):\n",
        "        test_list.append(f'{DATASET_NAME_3}/{image}')\n",
        "    \n",
        "    labels = np.append(np.zeros(len_live), np.ones(len(test_list)-len_live))\n",
        "    path_labels = dict(zip(test_list, labels))\n",
        "    testing_set = MyDataset(test_list, path_labels)\n",
        "    training_set = None\n",
        "    validation_set = None"
      ],
      "metadata": {
        "id": "uzw5hAG2H4Y2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4281fa-2ad9-49f0-a956-ff289f5cf967"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOPREDICT = 'to_predict'\n",
        "# with ZipFile(f'/content/gdrive/MyDrive/{TOPREDICT}.zip', 'r') as dataset_zip:\n",
        "#     dataset_zip.extractall('/content')\n",
        "\n",
        "# to predict\n",
        "predict_list = []\n",
        "for image in sorted(os.listdir(TOPREDICT)):\n",
        "    predict_list.append(f'{TOPREDICT}/{image}')\n",
        "\n",
        "labels = np.array([0, 1, 1])\n",
        "path_labels = dict(zip(predict_list, labels))\n",
        "predict_set = MyDataset(predict_list, path_labels)"
      ],
      "metadata": {
        "id": "8hLtW_UYysD0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions to train, test and log"
      ],
      "metadata": {
        "id": "7-vHXpEfJD1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(model, train_loader, valid_loader, loss_function, optimizer, scheduler, config):\n",
        "    \"\"\"\n",
        "    (count_of_epoch, batch_size, dataset, model, loss_function, optimizer, lr = 0.001)\n",
        "    trainer итерируется по кол-ву эпох и вызывает функцию train_epoch\n",
        "    count_of_epoch - кол-во эпох\n",
        "    batch_size - размер батча\n",
        "    dataset - данные для обучения\n",
        "    model - модель нейронной сети\n",
        "    loss_function - функция потерь\n",
        "    optimizer - оптимизатор\n",
        "    lr - скорость обучения, по умолчанию 0.001\n",
        "    \"\"\"\n",
        "    min_valid_loss = np.inf\n",
        "\n",
        "    # # in this foulder will save model weights\n",
        "    if not os.path.exists('/content/model_weights'):\n",
        "        os.mkdir('/content/model_weights')\n",
        "\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, loss_function, log=\"all\", log_freq=10)\n",
        "    \n",
        "    for e in range(config.count_of_epoch):\n",
        "        # train\n",
        "        epoch_loss = train_epoch(train_generator=train_loader, \n",
        "                    model=model, \n",
        "                    loss_function=loss_function, \n",
        "                    optimizer=optimizer)\n",
        "\n",
        "        # valid\n",
        "        valid_loss = 0.0\n",
        "        model.eval()\n",
        "        valid_loss = train_epoch(train_generator=valid_loader, \n",
        "                    model=model, \n",
        "                    loss_function=loss_function, \n",
        "                    optimizer=optimizer)\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        # log things\n",
        "        trainer_log(epoch_loss, valid_loss, e, scheduler.get_last_lr()[0], min_valid_loss)\n",
        "\n",
        "        # saving models\n",
        "        if min_valid_loss > valid_loss:\n",
        "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "            min_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), f'/content/model_weights/saved_model_{e}.pth')\n",
        "            wandb.log_artifact(f'/content/model_weights/saved_model_{e}.pth', \n",
        "                               name=f'saved_model_{e}', type='model')\n",
        "        print()\n",
        "\n",
        "def train_epoch(train_generator, model, loss_function, optimizer):\n",
        "    \"\"\"\n",
        "    внутри train_epoch итерируемся по батчам внутри батчгенератора\n",
        "    train_generator - батчгенератора\n",
        "    model - модель нейронной сети\n",
        "    loss_function - функция потерь\n",
        "    optimizer - оптимизатор\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    total = 0\n",
        "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
        "        batch_loss = train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
        "            \n",
        "        epoch_loss += batch_loss*len(batch_of_x)\n",
        "        total += len(batch_of_x)\n",
        "        #\n",
        "        print(total)\n",
        "        #\n",
        "    \n",
        "    return epoch_loss/total\n",
        "\n",
        "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
        "    \"\"\"\n",
        "    в train_on_batch обучаемся на одном батче\n",
        "    model - модель нейронной сети\n",
        "    x_batch - фичи\n",
        "    y_batch - таргеты(метки классов)\n",
        "    optimizer - оптимизатор\n",
        "    loss_function - функция потерь\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model(x_batch.to(device))\n",
        "    \n",
        "    loss = loss_function(output, y_batch.to(device))\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    return loss.cpu().item()\n",
        "\n",
        "def tester(model, test_loader):\n",
        "    pred = []\n",
        "    real = [] \n",
        "    model.eval()\n",
        "    for it, (x_batch, y_batch) in enumerate(test_loader):\n",
        "        x_batch = x_batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(x_batch)\n",
        "\n",
        "        pred.extend(torch.argmax(output, dim=-1).cpu().numpy().tolist())\n",
        "        real.extend(y_batch.cpu().numpy().tolist())\n",
        "\n",
        "    wandb.log({\"test_accuracy\": accuracy_score(real, pred)})\n",
        "\n",
        "    print(classification_report(real, pred, zero_division = 0))\n",
        "\n",
        "    print(confusion_matrix(real, pred))\n",
        "\n",
        "def predicter(model, predict_loader):\n",
        "    pred = []\n",
        "    real = []\n",
        "    model.eval()\n",
        "    for it, (x_batch, y_batch) in enumerate(predict_loader):\n",
        "        x_batch = x_batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(x_batch)\n",
        "\n",
        "        pred.extend(torch.argmax(output, dim=-1).cpu().numpy().tolist())\n",
        "        real.extend(y_batch.cpu().numpy().tolist())\n",
        "\n",
        "    np.save('predicted_labels.npy', np.array(pred))\n",
        "    print(pred[:10], '- predicted')\n",
        "    print(real[:10], '- real label')\n",
        "\n",
        "def trainer_log(train_loss, valid_loss, epoch, lr, min_val_loss):\n",
        "    wandb.log({'train_loss': train_loss, 'valid_loss': valid_loss,\n",
        "               'epoch': epoch, 'learning_rate': lr,\n",
        "               'min_validation_loss': min_val_loss})\n",
        "    print(f'train loss on {str(epoch).zfill(3)} epoch: {train_loss:.6f} with lr: {lr:.6f}')\n",
        "    print(f'valid loss on {str(epoch).zfill(3)} epoch: {valid_loss:.6f}')\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size, \n",
        "                                         shuffle=True,\n",
        "                                         pin_memory=True, num_workers=2)\n",
        "    return loader\n",
        "\n",
        "def download_folder_in_zip(dir_to_zip, output_filename, delete_dir_after_download=False):\n",
        "    os.system( \"zip -r {} {}\".format(output_filename, dir_to_zip))\n",
        "    if delete_dir_after_download:\n",
        "        os.system( \"rm -r {}\".format(dir_to_zip))\n",
        "    files.download(output_filename)"
      ],
      "metadata": {
        "id": "SBjpHMOgod76"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model pipeline"
      ],
      "metadata": {
        "id": "79TWUE15JXzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(hyperparameters, saved_model=None, to_train=True, to_test=True, to_predict=False):\n",
        "\n",
        "    with wandb.init(project=hyperparameters['project'], config=hyperparameters) as run:\n",
        "      config = wandb.config\n",
        "      \n",
        "      # build the model\n",
        "      model = build_model(run, config, saved_model)\n",
        "\n",
        "      # make the data and optimization \n",
        "      train_loader, valid_loader, test_loader, predict_loader, criterion, optimizer, scheduler = make(model, config)\n",
        "\n",
        "      print('config:', '\\n', config, '\\n', model, '\\n', 'running on device:', device, '\\n')\n",
        "\n",
        "      if to_train:\n",
        "        trainer(model, train_loader, valid_loader, criterion, optimizer, scheduler, config)\n",
        "\n",
        "      if to_test:\n",
        "        tester(model, test_loader)\n",
        "\n",
        "      if to_predict:\n",
        "          predicter(model, predict_loader)\n",
        "    return model\n",
        "\n",
        "def build_model(run, config, saved_model=None):\n",
        "    IN, H1, H2, H3, H4, H5, OUT = 177, 512, 256, 128, 64, 32, 2\n",
        "    p = config.dropout\n",
        "\n",
        "    model =  nn.Sequential(\n",
        "    nn.Linear(IN, H1), nn.Dropout(p), nn.BatchNorm1d(H1), nn.ReLU(),        \n",
        "    nn.Linear(H1, H2), nn.Dropout(p), nn.BatchNorm1d(H2), nn.ReLU(), \n",
        "    nn.Linear(H2, H3), nn.Dropout(p), nn.BatchNorm1d(H3), nn.ReLU(), \n",
        "    nn.Linear(H3, H4), nn.Dropout(p), nn.BatchNorm1d(H4), nn.ReLU(), \n",
        "    nn.Linear(H4, H5), nn.Dropout(p), nn.BatchNorm1d(H5), nn.ReLU(), \n",
        "    nn.Linear(H5, OUT), nn.Dropout(p), nn.BatchNorm1d(OUT), nn.ReLU()) \n",
        "\n",
        "    if saved_model == None:\n",
        "        # just init with some weights\n",
        "        def init_weights(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "        model.apply(init_weights)\n",
        "\n",
        "    else:\n",
        "        # download weights of saved model \n",
        "        # artifact = run.use_artifact(f'lanit-summer2022-antispoofing/{config.project}/{saved_model[0]}:v{saved_model[1]}', type='model')\n",
        "        # artifact_dir = artifact.download()\n",
        "        # model_path = os.path.join(artifact_dir, f'{saved_model[0]}.pth')\n",
        "\n",
        "        #\n",
        "        model_path = saved_model\n",
        "        #\n",
        "        model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make(model, config):\n",
        "    if training_set is not None: # if to train and test\n",
        "        train_loader = make_loader(training_set, batch_size=config.batch_size)\n",
        "        valid_loader = make_loader(validation_set, batch_size=config.batch_size)\n",
        "        test_loader = make_loader(testing_set, batch_size=config.batch_size)\n",
        "        predict_loader = make_loader(predict_set, batch_size=config.batch_size)\n",
        "\n",
        "    else:  # if only to test\n",
        "        train_loader = None\n",
        "        valid_loader = None\n",
        "        test_loader = make_loader(testing_set, batch_size=config.batch_size)\n",
        "        predict_loader = make_loader(predict_set, batch_size=config.batch_size)\n",
        "    \n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "    scheduler = StepLR(optimizer, config.step_size, config.step_gamma)\n",
        "    \n",
        "    return train_loader, valid_loader, test_loader, predict_loader, criterion, optimizer, scheduler"
      ],
      "metadata": {
        "id": "0fUw2SVSJZGu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runnning"
      ],
      "metadata": {
        "id": "MdPsvXrIKLra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')\n",
        "saved_model = '/content/gdrive/MyDrive/saved_model_128_256.pth'"
      ],
      "metadata": {
        "id": "8TCf84guUcjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bf38c9-91e3-48d4-bb87-12fd5239c8b4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(count_of_epoch=1, batch_size=1, lr=10, \n",
        "              dropout=0.0, critirion='CrossEntropyLoss', \n",
        "              optimizer='SGD', scheduler='StepLR', \n",
        "              step_size = 10, step_gamma = 0.5,\n",
        "              project='testing', name_of_model='mlp')\n",
        "\n",
        "\n",
        "model = pipeline(config, saved_model=saved_model, to_train=False, to_test=False, to_predict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714,
          "referenced_widgets": [
            "3a19b89c69ef4883980b8af23507da4c",
            "80f957ed590c4e1f9b4996ab3b907714",
            "32f2a17acbaf4fb7a46fb10e4593aa90",
            "6c2452a01a3f42bab31213b28669077a",
            "79d7887e8bce4523ac683a42710463d4",
            "aa08823834e34c8aa055d24133b63139",
            "62332f37b65042a0bea8d1b4c93b8c6a",
            "61a797b7434c4d5687be461ddaf6506b"
          ]
        },
        "id": "Kv5KQeqaKNH8",
        "outputId": "6d9b794e-5fb3-4008-cb99-97a472464e6d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220907_073231-2l7y71sg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/lanit-summer2022-antispoofing/testing/runs/2l7y71sg\" target=\"_blank\">exalted-butterfly-22</a></strong> to <a href=\"https://wandb.ai/lanit-summer2022-antispoofing/testing\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config: \n",
            " {'count_of_epoch': 1, 'batch_size': 1, 'lr': 10, 'dropout': 0.0, 'critirion': 'CrossEntropyLoss', 'optimizer': 'SGD', 'scheduler': 'StepLR', 'step_size': 10, 'step_gamma': 0.5, 'project': 'testing', 'name_of_model': 'mlp'} \n",
            " Sequential(\n",
            "  (0): Linear(in_features=177, out_features=512, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            "  (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (5): Dropout(p=0.0, inplace=False)\n",
            "  (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (7): ReLU()\n",
            "  (8): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (9): Dropout(p=0.0, inplace=False)\n",
            "  (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): ReLU()\n",
            "  (12): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (13): Dropout(p=0.0, inplace=False)\n",
            "  (14): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (15): ReLU()\n",
            "  (16): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (17): Dropout(p=0.0, inplace=False)\n",
            "  (18): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (19): ReLU()\n",
            "  (20): Linear(in_features=32, out_features=2, bias=True)\n",
            "  (21): Dropout(p=0.0, inplace=False)\n",
            "  (22): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (23): ReLU()\n",
            ") \n",
            " running on device: cpu \n",
            "\n",
            "[1, 1, 0] - predicted\n",
            "[1, 1, 0] - real label\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a19b89c69ef4883980b8af23507da4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">exalted-butterfly-22</strong>: <a href=\"https://wandb.ai/lanit-summer2022-antispoofing/testing/runs/2l7y71sg\" target=\"_blank\">https://wandb.ai/lanit-summer2022-antispoofing/testing/runs/2l7y71sg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220907_073231-2l7y71sg/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}