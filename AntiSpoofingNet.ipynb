{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrKJNWVWlF6r"
      },
      "source": [
        "### Imports and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pWYe2SeBlANj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import feature as skif\n",
        "import cv2\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIICsVak7Tq2",
        "outputId": "3b72bd75-1f4e-495e-83ea-91ff1e48fe98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.7/dist-packages (1.12.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.12.2->onnx) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxsim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9aF7aof7l0i",
        "outputId": "f96dbd4a-ab94-486d-9611-e61ff2a6d370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnxsim in /usr/local/lib/python3.7/dist-packages (0.4.8)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.7/dist-packages (from onnxsim) (12.5.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.7/dist-packages (from onnxsim) (1.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx->onnxsim) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx->onnxsim) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx->onnxsim) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.12.2->onnx->onnxsim) (1.15.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->onnxsim) (2.6.1)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich->onnxsim) (0.9.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETmyzN0l72lV",
        "outputId": "70f21c40-2bef-4eaa-8ad2-51f88a21bcdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.7/dist-packages (1.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (21.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (2.0.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.7.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.21.6)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.7/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->onnxruntime) (3.0.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->onnxruntime) (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnxsim"
      ],
      "metadata": {
        "id": "JvonMJI77cpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Gx0FpQgpxjcL",
        "outputId": "97db4f66-e648-49bc-e0c0-8c0665966b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -Uq"
      ],
      "metadata": {
        "id": "wagmMBD2q4lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZcwW43JrD6F",
        "outputId": "1508786f-797a-46bf-c67c-340d4a37d721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabletobetable\u001b[0m (\u001b[33mlanit-summer2022-antispoofing\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rane9HMJl29j"
      },
      "source": [
        "load dataset in files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aJhLISPlO3a",
        "outputId": "8ad9ed76-d65c-4211-9095-8d3883e14d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "DATASET_NAME = 'data256'\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "with ZipFile(f'/content/gdrive/MyDrive/{DATASET_NAME}.zip', 'r') as dataset_zip:\n",
        "    dataset_zip.extractall('/content')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')\n",
        "with ZipFile(f'/content/gdrive/MyDrive/features_128.zip', 'r') as dataset_zip:\n",
        "    dataset_zip.extractall('/content')\n",
        "\n",
        "with ZipFile(f'/content/gdrive/MyDrive/features_256.zip', 'r') as dataset_zip:\n",
        "    dataset_zip.extractall('/content')"
      ],
      "metadata": {
        "id": "JfsCMOkmDKxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in this foulder will save features\n",
        "if not os.path.exists('/content/features'):\n",
        "    os.mkdir('/content/features')\n",
        "\n",
        "# test\n",
        "feature_128 = np.load('/content/features_128/test_feature.npy', allow_pickle=True)\n",
        "feature_256 = np.load('/content/features_256/test_feature.npy', allow_pickle=True)\n",
        "test_features = np.append(feature_128, feature_256)\n",
        "np.save('features/test_feature.npy', test_features)\n",
        "\n",
        "# train\n",
        "feature_128 = np.load('/content/features_128/train_feature.npy', allow_pickle=True)\n",
        "feature_256 = np.load('/content/features_256/train_feature.npy', allow_pickle=True)\n",
        "train_features = np.append(feature_128, feature_256)\n",
        "\n",
        "np.save('features/train_feature.npy', train_features)"
      ],
      "metadata": {
        "id": "7ZzQOCX1Xtz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOLB9YsKr6Df"
      },
      "source": [
        "preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpekffFwsNVt"
      },
      "outputs": [],
      "source": [
        "def lbp_histogram(image,P=8,R=1,method = 'nri_uniform'):\n",
        "    '''\n",
        "    image: shape is N*M \n",
        "    '''\n",
        "    lbp = skif.local_binary_pattern(image, P,R, method) # lbp.shape is equal image.shape\n",
        "    # cv2.imwrite(\"lbp.png\",lbp)\n",
        "    max_bins = int(lbp.max() + 1) # max_bins is related P\n",
        "    hist,_= np.histogram(lbp, density=True, bins=max_bins, range=(0, max_bins))\n",
        "    return hist\n",
        "\n",
        "def save_features(file_list,file_name):\n",
        "    feature_label = []\n",
        "    for line in open(file_list):\n",
        "        image_path = line.strip().split(' ')[0]\n",
        "        label = int(line.strip().split(' ')[1])\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
        "        y_h = lbp_histogram(image[:,:,0]) # y channel\n",
        "        cb_h = lbp_histogram(image[:,:,1]) # cb channel\n",
        "        cr_h = lbp_histogram(image[:,:,2]) # cr channel\n",
        "        feature = np.concatenate((y_h,cb_h,cr_h))\n",
        "        feature_label.append(np.append(feature,np.array(label)))\n",
        "    np.save(file_name,np.array(feature_label, dtype=object)) #add 'dtype=object'\n",
        "\n",
        "def create_partition_and_labels(data1, data2):\n",
        "    \"\"\"\n",
        "    calculate lbp features and then\n",
        "    creates 3 dictionary like follow:\n",
        "\n",
        "    >>> partition\n",
        "    {'train': ['id-1', 'id-2', 'id-3'], 'validation': ['id-4']}\n",
        "\n",
        "    >>> ID_vectors\n",
        "    {'id-1': [...], 'id-2': [...], 'id-3': [...], 'id-4': [...]}\n",
        "\n",
        "    >>> ID_labels\n",
        "    {'id-1': 0, 'id-2': 1, 'id-3': 2, 'id-4': 1}\n",
        "    \"\"\"\n",
        "    train_list = []\n",
        "    test_list = []\n",
        "\n",
        "    # test data1\n",
        "    for folder in os.listdir(f'/content/{data1}/test'):\n",
        "        if os.path.exists(f'/content/{data1}/test/'+folder+'/live'):\n",
        "            for image in os.listdir(f'/content/{data1}/test/'+folder+'/live'):\n",
        "                test_list.append(f'\\n/content/{data1}/test/'+folder+'/live/'+image+' 0')\n",
        "        if os.path.exists(f'/content/{data1}/test/'+folder+'/spoof'):\n",
        "            for image in os.listdir(f'/content/{data1}/test/'+folder+'/spoof'):\n",
        "                test_list.append(f'\\n/content/{data1}/test/'+folder+'/spoof/'+image+' 1')\n",
        "    \n",
        "    # train data1\n",
        "    for folder in os.listdir(f'/content/{data1}/train'):\n",
        "        if os.path.exists(f'/content/{data1}/train/'+folder+'/live'):\n",
        "            for image in os.listdir(f'/content/{data1}/train/'+folder+'/live'):\n",
        "                train_list.append(f'\\n/content/{data1}/train/'+folder+'/live/'+image+' 0')\n",
        "        if os.path.exists(f'/content/{data1}/train/'+folder+'/spoof'):\n",
        "            for image in os.listdir(f'/content/{data1}/train/'+folder+'/spoof'):\n",
        "                train_list.append(f'\\n/content/{data1}/train/'+folder+'/spoof/'+image+' 1')\n",
        "\n",
        "\n",
        "    # test data2\n",
        "    for folder in os.listdir(f'/content/{data2}/test'):\n",
        "        if os.path.exists(f'/content/{data2}/test/'+folder+'/live'):\n",
        "            for image in os.listdir(f'/content/{data2}/test/'+folder+'/live'):\n",
        "                test_list.append(f'\\n/content/{data2}/test/'+folder+'/live/'+image+' 0')\n",
        "        if os.path.exists(f'/content/{data2}/test/'+folder+'/spoof'):\n",
        "            for image in os.listdir(f'/content/{data2}/test/'+folder+'/spoof'):\n",
        "                test_list.append(f'\\n/content/{data2}/test/'+folder+'/spoof/'+image+' 1')\n",
        "    \n",
        "    # train data2\n",
        "    for folder in os.listdir(f'/content/{data2}/train'):\n",
        "        if os.path.exists(f'/content/{data2}/train/'+folder+'/live'):\n",
        "            for image in os.listdir(f'/content/{data2}/train/'+folder+'/live'):\n",
        "                train_list.append(f'\\n/content/{data2}/train/'+folder+'/live/'+image+' 0')\n",
        "        if os.path.exists(f'/content/{data2}/train/'+folder+'/spoof'):\n",
        "            for image in os.listdir(f'/content/{data2}/train/'+folder+'/spoof'):\n",
        "                train_list.append(f'\\n/content/{data2}/train/'+folder+'/spoof/'+image+' 1')\n",
        "\n",
        "    ### GET FEATURES  ###\n",
        "\n",
        "    # remove simbol '\\n' from first string\n",
        "    test_list[0] = test_list[0][1::]\n",
        "    train_list[0] = train_list[0][1::]\n",
        "\n",
        "    # create files if they dont exists .txt\n",
        "    with open(\"/content/test_file_list.txt\", \"w+\") as textfile:\n",
        "        for line in test_list:\n",
        "            textfile.write(line)\n",
        "\n",
        "    with open(\"/content/train_file_list.txt\", \"w+\") as textfile:\n",
        "        for line in train_list:\n",
        "            textfile.write(line)\n",
        "\n",
        "    # in this foulder will save features\n",
        "    if not os.path.exists('/content/features'):\n",
        "        os.mkdir('/content/features')\n",
        "\n",
        "    # getting features\n",
        "    # save_features(\"test_file_list.txt\",\"features/test_feature.npy\")\n",
        "    # save_features(\"train_file_list.txt\",\"features/train_feature.npy\")\n",
        "\n",
        "    # load features (потом папку заменить просто на features)\n",
        "    test_npy = np.load('/content/features/test_feature.npy', allow_pickle=True)\n",
        "    train_npy = np.load('/content/features/train_feature.npy', allow_pickle=True)\n",
        "\n",
        "    # convert to pandas to preprocess data\n",
        "    test_features = pd.DataFrame(index=range(test_npy.shape[0]), columns=range(test_npy[0].shape[0]))\n",
        "    train_features = pd.DataFrame(index=range(train_npy.shape[0]), columns=range(train_npy[0].shape[0]))\n",
        "\n",
        "    # filter incorrect features\n",
        "    for i,line in enumerate(test_npy):\n",
        "        if line.shape == (178,):\n",
        "            test_features.loc[i] = line\n",
        "    for i,line in enumerate(train_npy):\n",
        "        if line.shape == (178,):\n",
        "            train_features.loc[i] = line\n",
        "\n",
        "    # drop nan values\n",
        "    test_features.dropna(inplace=True)\n",
        "    train_features.dropna(inplace=True)\n",
        "\n",
        "    # features labels split\n",
        "    test, test_labels = test_features.loc[:, :176], test_features.loc[:, 177].tolist()\n",
        "    train, train_labels = train_features.loc[:, :176], train_features.loc[:, 177].tolist()\n",
        "\n",
        "    # conver to list\n",
        "    test = test.to_numpy().tolist()\n",
        "    train = train.to_numpy().tolist()\n",
        "    \n",
        "    # train <-> valid split\n",
        "    train, valid, train_labels, valid_labels = train_test_split(train, train_labels, \n",
        "                                                                train_size = 0.8, random_state=44)\n",
        "\n",
        "    # train test split\n",
        "    partition = {'test': np.arange(len(test)), \n",
        "                 'train': np.arange(len(test), len(test + train)), \n",
        "                 'valid': np.arange(len(test + train), len(test + train + valid))}\n",
        "\n",
        "    # ID <-> image vector(feature)\n",
        "    ID_list = np.arange(len(test + train + valid))\n",
        "    image_vectors = test + train + valid\n",
        "    ID_vectors = dict(zip(ID_list, image_vectors))\n",
        "\n",
        "    # ID <-> image label\n",
        "    labels_list = test_labels + train_labels + valid_labels\n",
        "    ID_labels = dict(zip(ID_list, labels_list))\n",
        "\n",
        "    return partition, ID_vectors, ID_labels\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, partition, id_vectors, id_labels):\n",
        "        self.partition = partition\n",
        "        self.id_vectors = id_vectors\n",
        "        self.id_labels = id_labels\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.partition)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        id = self.partition[index]\n",
        "        item_vector= self.id_vectors[id]\n",
        "\n",
        "        X = item_vector\n",
        "        y = self.id_labels[id]\n",
        "\n",
        "        return torch.Tensor(X), torch.tensor(y, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create datasets"
      ],
      "metadata": {
        "id": "k5LEuKL_00-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK-w_vnOxGHk"
      },
      "outputs": [],
      "source": [
        "partition, id_vectors, id_labels = create_partition_and_labels(data1 = 'data128', data2 = 'data256')\n",
        "\n",
        "training_set = MyDataset(partition['train'], id_vectors, id_labels)\n",
        "validation_set = MyDataset(partition['valid'], id_vectors, id_labels)\n",
        "testing_set = MyDataset(partition['test'], id_vectors, id_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions to train, test and log"
      ],
      "metadata": {
        "id": "Vuu1o3na05IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(model, train_loader, valid_loader, loss_function, optimizer, scheduler, config):\n",
        "    \"\"\"\n",
        "    (count_of_epoch, batch_size, dataset, model, loss_function, optimizer, lr = 0.001)\n",
        "    trainer итерируется по кол-ву эпох и вызывает функцию train_epoch\n",
        "    count_of_epoch - кол-во эпох\n",
        "    batch_size - размер батча\n",
        "    dataset - данные для обучения\n",
        "    model - модель нейронной сети\n",
        "    loss_function - функция потерь\n",
        "    optimizer - оптимизатор\n",
        "    lr - скорость обучения, по умолчанию 0.001\n",
        "    \"\"\"\n",
        "    min_valid_loss = np.inf\n",
        "\n",
        "    # # in this foulder will save model weights\n",
        "    if not os.path.exists('/content/model_weights'):\n",
        "        os.mkdir('/content/model_weights')\n",
        "\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, loss_function, log=\"all\", log_freq=10)\n",
        "    \n",
        "    for e in range(config.count_of_epoch):\n",
        "        # train\n",
        "        epoch_loss = train_epoch(train_generator=train_loader, \n",
        "                    model=model, \n",
        "                    loss_function=loss_function, \n",
        "                    optimizer=optimizer)\n",
        "\n",
        "        # valid\n",
        "        valid_loss = 0.0\n",
        "        model.eval()\n",
        "        valid_loss = train_epoch(train_generator=valid_loader, \n",
        "                    model=model, \n",
        "                    loss_function=loss_function, \n",
        "                    optimizer=optimizer)\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        # log things\n",
        "        trainer_log(epoch_loss, valid_loss, e, scheduler.get_last_lr()[0], min_valid_loss)\n",
        "\n",
        "        # saving models\n",
        "        if min_valid_loss > valid_loss:\n",
        "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "            min_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), f'/content/model_weights/saved_model_{e}.pth')\n",
        "            wandb.log_artifact(f'/content/model_weights/saved_model_{e}.pth', \n",
        "                               name=f'saved_model_{e}', type='model')\n",
        "        print()\n",
        "\n",
        "def train_epoch(train_generator, model, loss_function, optimizer):\n",
        "    \"\"\"\n",
        "    внутри train_epoch итерируемся по батчам внутри батчгенератора\n",
        "    train_generator - батчгенератора\n",
        "    model - модель нейронной сети\n",
        "    loss_function - функция потерь\n",
        "    optimizer - оптимизатор\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    total = 0\n",
        "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
        "        batch_loss = train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
        "            \n",
        "        epoch_loss += batch_loss*len(batch_of_x)\n",
        "        total += len(batch_of_x)\n",
        "    \n",
        "    return epoch_loss/total\n",
        "\n",
        "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
        "    \"\"\"\n",
        "    в train_on_batch обучаемся на одном батче\n",
        "    model - модель нейронной сети\n",
        "    x_batch - фичи\n",
        "    y_batch - таргеты(метки классов)\n",
        "    optimizer - оптимизатор\n",
        "    loss_function - функция потерь\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model(x_batch.to(device))\n",
        "    \n",
        "    loss = loss_function(output, y_batch.to(device))\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    return loss.cpu().item()"
      ],
      "metadata": {
        "id": "SBjpHMOgod76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tester(run, model, test_loader):\n",
        "    pred = []\n",
        "    real = [] \n",
        "    model.eval()\n",
        "    for it, (x_batch, y_batch) in enumerate(test_loader):\n",
        "        x_batch = x_batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(x_batch)\n",
        "\n",
        "        pred.extend(torch.argmax(output, dim=-1).cpu().numpy().tolist())\n",
        "        real.extend(y_batch.cpu().numpy().tolist())\n",
        "\n",
        "    wandb.log({\"test_accuracy\": accuracy_score(real, pred)})\n",
        "\n",
        "    print(classification_report(real, pred, zero_division = 0))\n",
        "\n",
        "    # Save the model in the exchangeable ONNX format\n",
        "    f_pth = f'/content/model_weights/saved_model_latest.pth'\n",
        "    f_onnx = f_pth.replace('.pth', '.onnx')\n",
        "    torch.save(model.state_dict(), f_pth)\n",
        "    torch.onnx.export(model, \n",
        "                      x_batch,\n",
        "                      f_onnx, \n",
        "                      opset_version=11, \n",
        "                      input_names=['input'],\n",
        "                      output_names=['output'],\n",
        "                      training= torch.onnx.TrainingMode.EVAL,\n",
        "                      do_constant_folding=False, \n",
        "                      dynamic_axes=None)\n",
        "    \n",
        "    model_onnx = onnx.load(f_onnx)  # load onnx model\n",
        "    onnx.checker.check_model(model_onnx) \n",
        "    print(model_onnx.ir_version)\n",
        "\n",
        "    model_onnx, check = onnxsim.simplify(\n",
        "                        model_onnx,\n",
        "                        dynamic_input_shape=True,\n",
        "                        input_shapes={'input': list(x_batch.shape)} )\n",
        "\n",
        "    onnx.save(model_onnx, f_onnx)\n",
        "\n",
        "    wandb.save('/content/model_weights/saved_model_latest.onnx')"
      ],
      "metadata": {
        "id": "MMD_O3EwwaSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer_log(train_loss, valid_loss, epoch, lr, min_val_loss):\n",
        "    wandb.log({'train_loss': train_loss, 'valid_loss': valid_loss,\n",
        "               'epoch': epoch, 'learning_rate': lr,\n",
        "               'min_validation_loss': min_val_loss})\n",
        "    print(f'train loss on {str(epoch).zfill(3)} epoch: {train_loss:.6f} with lr: {lr:.6f}')\n",
        "    print(f'valid loss on {str(epoch).zfill(3)} epoch: {valid_loss:.6f}')\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size, \n",
        "                                         shuffle=True,\n",
        "                                         pin_memory=True, num_workers=2)\n",
        "    return loader\n",
        "\n",
        "def download_folder_in_zip(dir_to_zip, output_filename, delete_dir_after_download=False):\n",
        "    os.system( \"zip -r {} {}\".format(output_filename, dir_to_zip))\n",
        "    if delete_dir_after_download:\n",
        "        os.system( \"rm -r {}\".format(dir_to_zip))\n",
        "    files.download(output_filename)"
      ],
      "metadata": {
        "id": "xSvKzTeZ1vAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model pipeline"
      ],
      "metadata": {
        "id": "L1G_gdfQ1Tif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(hyperparameters, saved_model=None, to_train=True, to_test=True):\n",
        "\n",
        "    with wandb.init(project=hyperparameters['project'], config=hyperparameters) as run:\n",
        "      config = wandb.config\n",
        "\n",
        "      # build the model\n",
        "      model = build_model(run, config, saved_model)\n",
        "\n",
        "      # make the data and optimization \n",
        "      train_loader, valid_loader, test_loader, criterion, optimizer, scheduler = make(model, config)\n",
        "\n",
        "      print('config:', '\\n', config, '\\n', model, '\\n', 'running on device:', device, '\\n')\n",
        "\n",
        "      if to_train:\n",
        "        trainer(model, train_loader, valid_loader, criterion, optimizer, scheduler, config)\n",
        "\n",
        "      if to_test:\n",
        "        tester(run, model, test_loader)\n",
        "    return model\n",
        "\n",
        "def build_model(run, config, saved_model=None):\n",
        "    IN, H1, H2, H3, H4, H5, OUT = 177, 512, 256, 128, 64, 32, 2\n",
        "    p = config.dropout\n",
        "\n",
        "    model =  nn.Sequential(\n",
        "    nn.Linear(IN, H1), nn.Dropout(p), nn.BatchNorm1d(H1), nn.ReLU(),        \n",
        "    nn.Linear(H1, H2), nn.Dropout(p), nn.BatchNorm1d(H2), nn.ReLU(), \n",
        "    nn.Linear(H2, H3), nn.Dropout(p), nn.BatchNorm1d(H3), nn.ReLU(), \n",
        "    nn.Linear(H3, H4), nn.Dropout(p), nn.BatchNorm1d(H4), nn.ReLU(), \n",
        "    nn.Linear(H4, H5), nn.Dropout(p), nn.BatchNorm1d(H5), nn.ReLU(), \n",
        "    nn.Linear(H5, OUT), nn.Dropout(p), nn.BatchNorm1d(OUT), nn.ReLU()) \n",
        "\n",
        "    if saved_model == None:\n",
        "        # just init with some weights\n",
        "        def init_weights(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "        model.apply(init_weights)\n",
        "\n",
        "    else:\n",
        "        # download weights of saved model \n",
        "        artifact = run.use_artifact(f'lanit-summer2022-antispoofing/{config.project}/{saved_model[0]}:v{saved_model[1]}', type='model')\n",
        "        artifact_dir = artifact.download()\n",
        "        model_path = os.path.join(artifact_dir, f'{saved_model[0]}.pth')\n",
        "        #\n",
        "        model_path = '/content/model_weights/saved_model_latest.onnx'\n",
        "        #\n",
        "        model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make(model, config):\n",
        "    train, valid, test = training_set, validation_set, testing_set\n",
        "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
        "    valid_loader = make_loader(valid, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "    scheduler = StepLR(optimizer, config.step_size, config.step_gamma)\n",
        "    \n",
        "    return train_loader, valid_loader, test_loader, criterion, optimizer, scheduler"
      ],
      "metadata": {
        "id": "9SrdTbuwsugO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running"
      ],
      "metadata": {
        "id": "72MGje2q17rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(count_of_epoch=1, batch_size=1024, lr=9, \n",
        "              dropout=0.1, critirion='CrossEntropyLoss', \n",
        "              optimizer='SGD', scheduler='StepLR', \n",
        "              step_size = 5, step_gamma = 0.1,\n",
        "              project='experiments', name_of_model='mlp', data='128+256')\n",
        "\n",
        "# to train and test model\n",
        "model = pipeline(config, to_train=True, to_test=True)"
      ],
      "metadata": {
        "id": "DFV9inLo8lhi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0c24d4573ba446a784071b1b47538476",
            "567950c7caa7445e83ba0ded515d5058",
            "d8dbd66087074e998fa62c1c9c325850",
            "4e4165d914d64c9796b95f010e7f7215",
            "9b07871854534277a0728460921befa4",
            "8edfd7427460447e9b7ed6d62a6e5b64",
            "330885aa87c6450389857622297252d2",
            "e2ffd0cffaa34a83a154d8081198ee42"
          ]
        },
        "outputId": "815eed86-8d9f-4534-cf3e-2b0a96efbd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220901_161342-1ys44yeq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/lanit-summer2022-antispoofing/experiments/runs/1ys44yeq\" target=\"_blank\">super-cosmos-85</a></strong> to <a href=\"https://wandb.ai/lanit-summer2022-antispoofing/experiments\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config: \n",
            " {'count_of_epoch': 1, 'batch_size': 1024, 'lr': 9, 'dropout': 0.1, 'critirion': 'CrossEntropyLoss', 'optimizer': 'SGD', 'scheduler': 'StepLR', 'step_size': 5, 'step_gamma': 0.1, 'project': 'experiments', 'name_of_model': 'mlp', 'data': '128+256'} \n",
            " Sequential(\n",
            "  (0): Linear(in_features=177, out_features=512, bias=True)\n",
            "  (1): Dropout(p=0.1, inplace=False)\n",
            "  (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (5): Dropout(p=0.1, inplace=False)\n",
            "  (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (7): ReLU()\n",
            "  (8): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (9): Dropout(p=0.1, inplace=False)\n",
            "  (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): ReLU()\n",
            "  (12): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (13): Dropout(p=0.1, inplace=False)\n",
            "  (14): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (15): ReLU()\n",
            "  (16): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (17): Dropout(p=0.1, inplace=False)\n",
            "  (18): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (19): ReLU()\n",
            "  (20): Linear(in_features=32, out_features=2, bias=True)\n",
            "  (21): Dropout(p=0.1, inplace=False)\n",
            "  (22): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (23): ReLU()\n",
            ") \n",
            " running on device: cpu \n",
            "\n",
            "train loss on 000 epoch: 0.531518 with lr: 9.000000\n",
            "valid loss on 000 epoch: 0.498736\n",
            "Validation Loss Decreased(inf--->0.498736) \t Saving The Model\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.68      0.69     39840\n",
            "           1       0.79      0.81      0.80     59446\n",
            "\n",
            "    accuracy                           0.76     99286\n",
            "   macro avg       0.75      0.74      0.74     99286\n",
            "weighted avg       0.75      0.76      0.75     99286\n",
            "\n",
            "6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mWARNING: The argument `dynamic_input_shape=True` is not needed any more, onnxsim can now support dynamic input \u001b[0m\n",
              "\u001b[1;31mshapes natively, please refer to the latest documentation. An error will be raised in the future.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">WARNING: The argument `dynamic_input_shape=True` is not needed any more, onnxsim can now support dynamic input </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">shapes natively, please refer to the latest documentation. An error will be raised in the future.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mWARNING: The argument `input_shapes` is deprecated. Please use `overwrite_input_shapes` and/or `test_input_shapes` \u001b[0m\n",
              "\u001b[1;31minstead. An error will be raised in the future.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">WARNING: The argument `input_shapes` is deprecated. Please use `overwrite_input_shapes` and/or `test_input_shapes` </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">instead. An error will be raised in the future.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='1.041 MB of 2.073 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.502095…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c24d4573ba446a784071b1b47538476"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>valid_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>learning_rate</td><td>9</td></tr><tr><td>min_validation_loss</td><td>inf</td></tr><tr><td>test_accuracy</td><td>0.75549</td></tr><tr><td>train_loss</td><td>0.53152</td></tr><tr><td>valid_loss</td><td>0.49874</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">super-cosmos-85</strong>: <a href=\"https://wandb.ai/lanit-summer2022-antispoofing/experiments/runs/1ys44yeq\" target=\"_blank\">https://wandb.ai/lanit-summer2022-antispoofing/experiments/runs/1ys44yeq</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220901_161342-1ys44yeq/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c24d4573ba446a784071b1b47538476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_567950c7caa7445e83ba0ded515d5058",
              "IPY_MODEL_d8dbd66087074e998fa62c1c9c325850"
            ],
            "layout": "IPY_MODEL_4e4165d914d64c9796b95f010e7f7215"
          }
        },
        "567950c7caa7445e83ba0ded515d5058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b07871854534277a0728460921befa4",
            "placeholder": "​",
            "style": "IPY_MODEL_8edfd7427460447e9b7ed6d62a6e5b64",
            "value": "2.168 MB of 2.168 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "d8dbd66087074e998fa62c1c9c325850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_330885aa87c6450389857622297252d2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2ffd0cffaa34a83a154d8081198ee42",
            "value": 1
          }
        },
        "4e4165d914d64c9796b95f010e7f7215": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b07871854534277a0728460921befa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8edfd7427460447e9b7ed6d62a6e5b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "330885aa87c6450389857622297252d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2ffd0cffaa34a83a154d8081198ee42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}